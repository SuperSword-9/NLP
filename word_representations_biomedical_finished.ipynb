{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation in Biomedical Domain\n",
    "\n",
    "Before you start, please make sure you have read this notebook. You are encouraged to follow the recommendations but you are also free to develop your own solution from scratch. \n",
    "\n",
    "## Marking Scheme\n",
    "\n",
    "- Biomedical imaging project: 40%\n",
    "    - 20%: accuracy of the final model on the test set\n",
    "    - 20%: rationale of model design and final report\n",
    "- Natural language processing project: 40%\n",
    "    - 30%: completeness of the project\n",
    "    - 10%: final report\n",
    "- Presentation skills and team work: 20%\n",
    "\n",
    "\n",
    "This project forms 40\\% of the total score for summer/winter school. The marking scheme of each part of this project is provided below with a cap of 100\\%.\n",
    "\n",
    "You are allowed to use open source libraries as long as the libraries are properly cited in the code and final report. The usage of third-party code without proper reference will be treated as plagiarism, which will not be tolerated.\n",
    "\n",
    "You are encouraged to develop the algorithms by yourselves (without using third-party code as much as possible). We will factor such effort into the marking process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites \n",
    "\n",
    "Recommended environment\n",
    "\n",
    "- Python 3.7 or newer\n",
    "- Free disk space: 100GB\n",
    "\n",
    "Download the data\n",
    "\n",
    "```sh\n",
    "# navigate to the data folder\n",
    "cd data\n",
    "\n",
    "# download the data file\n",
    "# which is also available at https://www.semanticscholar.org/cord19/download\n",
    "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-07-26/document_parses.tar.gz\n",
    "\n",
    "# decompress the file which may take several minutes\n",
    "tar -xf document_parses.tar.gz\n",
    "\n",
    "# which creates a folder named document_parses\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (20%): Parse the Data\n",
    "\n",
    "The JSON files are located in two sub-folders in `document_parses`. You will need to scan all JSON files and extract text (i.e. `string`) from relevant fields (e.g. body text, abstract, titles).\n",
    "\n",
    "You are encouraged to extract full article text from body text if possible. If the hardware resource is limited, you can extract from abstract or titles as alternatives. \n",
    "\n",
    "Note: The number of JSON files is around 425k so it may take more than 10 minutes to parse all documents.\n",
    "\n",
    "For more information about the dataset: https://www.semanticscholar.org/cord19/download\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A list of text (`string`) extracted from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# 设置数据路径和输出文件路径\n",
    "pdf_json_dir = r\"D:\\Scholar\\AI Winter School 2025\\Project\\NLP\\document_parses\\pdf_json\"\n",
    "pmc_json_dir = r\"D:\\Scholar\\AI Winter School 2025\\Project\\NLP\\document_parses\\pmc_json\"\n",
    "output_txt = r\"D:\\Scholar\\AI Winter School 2025\\Project\\Ours\\NLP\\Text.txt\"\n",
    "\n",
    "# 获取所有 JSON 文件路径\n",
    "json_files = glob.glob(os.path.join(pdf_json_dir, \"*.json\")) + glob.glob(os.path.join(pmc_json_dir, \"*.json\"))\n",
    "\n",
    "# 提取内容的函数\n",
    "def extract_content(json_files, output_txt):\n",
    "    with open(output_txt, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        for file_path in json_files:\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    \n",
    "                    # 提取标题\n",
    "                    title = json_data.get(\"metadata\", {}).get(\"title\", \"\").strip()\n",
    "                    if title:\n",
    "                        output_file.write(f\"{title}\\n\")\n",
    "                    \n",
    "                    # 提取正文文本\n",
    "                    body_texts = json_data.get(\"body_text\", [])\n",
    "                    for body in body_texts:\n",
    "                        text = body.get(\"text\", \"\")\n",
    "                        cite_spans = body.get(\"cite_spans\", [])\n",
    "                        \n",
    "                        # 删除 cite_spans 中的引用内容\n",
    "                        for cite in cite_spans:\n",
    "                            cite_text = cite.get(\"text\", \"\")\n",
    "                            text = text.replace(cite_text, \"\")\n",
    "                        \n",
    "                        if text:\n",
    "                            output_file.write(f\"{text}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# 调用函数提取内容并写入到 .txt 文件中\n",
    "extract_content(json_files, output_txt)\n",
    "print(f\"内容已提取并保存到 {output_txt}\")\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (30%): Tokenization\n",
    "\n",
    "Traverse the extracted text and segment the text into words (or tokens).\n",
    "\n",
    "The following tracks can be developed in independentely. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- Tokenizer(s) that is able to tokenize any input text.\n",
    "\n",
    "Note: Because of the computation complexity of tokenizers, it may take hours/days to process all documents. Which tokenizer is more efficient? Any idea to speedup?\n",
    "\n",
    "### Track 2.1 (10%): Use split()\n",
    "\n",
    "Use the standard `split()` by Python.\n",
    "\n",
    "### Track 2.2 (10%): Use NLTK or SciSpaCy\n",
    "\n",
    "NLTK tokenizer: https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "SciSpaCy: https://github.com/allenai/scispacy\n",
    "\n",
    "Note: You may need to install NLTK and SpaCy so please refer to their websites for installation instructions.\n",
    "\n",
    "### Track 2.3 (10%): Use Byte-Pair Encoding (BPE)\n",
    "\n",
    "Byte-Pair Encoding (BPE): https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "Note: You may need to install Huggingface's transformers so please refer to its website for installation instructions.\n",
    "\n",
    "### Track 2.4 (Bonus +5%): Build new Byte-Pair Encoding (BPE)\n",
    "\n",
    "This track may be dependent on track 2.3.\n",
    "\n",
    "The above pre-built tokenization methods may not be suitable for biomedical domain as the words/tokens (e.g. diseases, sympotoms, chemicals, medications, phenotypes, genotypes etc.) can be very different from the words/tokens commonly used in daily life. Can you build and train a new BPE model for biomedical domain in particular?\n",
    "\n",
    "### Open Question (Optional):\n",
    "\n",
    "- What are the pros and cons of the above tokenizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "import os\n",
    "import nltk\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# 设置输入文件路径\n",
    "txt_file = r\"E:\\NLP\\Text_part_1.txt\"\n",
    "bpe_model_dir = r\"E:\\NLP\\bpe_model\"\n",
    "\n",
    "# 方法1：使用 Python 提供的 split() 函数\n",
    "def split_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# 方法2：使用 NLTK\n",
    "nltk.download('punkt')\n",
    "def nltk_tokenizer(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# 方法3：使用 GPT2TokenizerFast 进行 Byte-Pair Encoding (BPE)\n",
    "def load_gpt2_tokenizer():\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    return tokenizer\n",
    "\n",
    "# 加载 GPT2 分词器\n",
    "gpt2_tokenizer = load_gpt2_tokenizer()\n",
    "\n",
    "def gpt2_tokenizer_func(text):\n",
    "    tokens = gpt2_tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# 逐行读取文件并进行分词处理\n",
    "def process_file_line_by_line(file_path, tokenizer_func):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tokens = tokenizer_func(line)\n",
    "            print(tokens[:20])  # 打印前20个token\n",
    "\n",
    "# 示例使用\n",
    "print(\"split() 分词结果：\")\n",
    "process_file_line_by_line(txt_file, split_tokenizer)\n",
    "\n",
    "print(\"NLTK 分词结果：\")\n",
    "process_file_line_by_line(txt_file, nltk_tokenizer)\n",
    "\n",
    "print(\"GPT2 分词结果：\")\n",
    "process_file_line_by_line(txt_file, gpt2_tokenizer_func)\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# 设置输入文件路径\n",
    "txt_file = r\"E:\\NLP\\Text_part_1.txt\"\n",
    "bpe_model_dir = r\"E:\\NLP\\bpe_model\"\n",
    "output_file = r\"E:\\NLP\\tokens.txt\"\n",
    "vocab_file = r\"E:\\NLP\\vocabulary.txt\"\n",
    "\n",
    "# 创建保存目录\n",
    "os.makedirs(bpe_model_dir, exist_ok=True)\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# 设置标准化器和预分词器\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 设置训练器\n",
    "trainer = BpeTrainer(vocab_size=10000, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "\n",
    "# 读取训练数据并过滤非 ASCII 可打印字符\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 过滤非 ASCII 可打印字符\n",
    "filtered_lines = [re.sub(r'[^\\x20-\\x7E]', '', line) for line in lines]\n",
    "\n",
    "# 训练分词器\n",
    "tokenizer.train_from_iterator(filtered_lines, trainer=trainer)\n",
    "\n",
    "# 设置后处理器\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    pair=\"<s> $A </s> <s> $B </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 保存分词器\n",
    "tokenizer.save(os.path.join(bpe_model_dir, \"bpe_tokenizer.json\"))\n",
    "\n",
    "# 获取词汇表并写入文件\n",
    "vocab = tokenizer.get_vocab()\n",
    "with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, (token, token_id) in enumerate(vocab.items()):\n",
    "        f.write(f\"{idx + 1}: {token}\\n\")\n",
    "\n",
    "# 打印词汇总数\n",
    "print(f\"Total vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# 使用分词器进行分词\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == '__main__':\n",
    "    sample_text = \"this is a sample text, let's see how the tokenizer works.\"\n",
    "    tokens = tokenize_text(sample_text)\n",
    "    print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# 设置文件路径\n",
    "bpe_model_dir = r\"E:\\NLP\\bpe_model\"\n",
    "# input_file = r\"D:\\Scholar\\AI Winter School 2025\\Project\\Ours\\NLP\\Text.txt\"\n",
    "input_file = r\"E:\\NLP\\Text_part_1.txt\"\n",
    "output_file = r\"E:\\NLP\\test_tokens.txt\"\n",
    "vocab_file = r\"E:\\NLP\\vocabulary.txt\"\n",
    "\n",
    "# 加载训练好的分词器\n",
    "tokenizer = Tokenizer.from_file(os.path.join(bpe_model_dir, \"bpe_tokenizer.json\"))\n",
    "\n",
    "# 加载词汇表\n",
    "with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = set(line.split(\": \")[1].strip() for line in f)\n",
    "    \n",
    "# 使用分词器进行分词\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text).tokens\n",
    "    return [token for token in tokens if token in vocab and token.isascii() and not token.isdigit()]\n",
    "\n",
    "# 分批处理文本\n",
    "def process_in_batches(input_file, output_file, batch_size):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        while True:\n",
    "            lines = infile.readlines(batch_size)\n",
    "            if not lines:\n",
    "                break\n",
    "            text = \"\".join(lines)\n",
    "            tokens = tokenize_text(text)\n",
    "            outfile.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 16384\n",
    "    process_in_batches(input_file, output_file, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our thinkings：\n",
    "\n",
    "Each tokenizer in your code has its pros and cons, depending on what you need. The simplest method, using Python’s `split()` function, is fast and easy but doesn’t handle punctuation well. The `nltk_tokenizer` is a bit smarter, recognizing words more accurately, but it’s still quite basic.  \n",
    "\n",
    "For deep learning, `gpt2_tokenizer_func` is a strong choice since it uses Byte-Pair Encoding (BPE) to handle rare words efficiently. However, it can be slower and sometimes splits words awkwardly. Your custom BPE tokenizer (`3_my_tokenizer.py`) offers more control, cleaning the text and supporting special tokens, but it requires training and setup.  \n",
    "\n",
    "Finally, `4_tokenize.py` applies your trained tokenizer to real text, ensuring only valid words from the vocabulary are used. This keeps the output clean but may discard words not in the vocab.  \n",
    "\n",
    "If you need something simple, `split()` or `nltk_tokenizer` will work. If you’re dealing with AI models, `gpt2_tokenizer_func` is better. But for full customization, your custom BPE tokenizer is the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (30%): Build Word Representations\n",
    "\n",
    "Build word representations for each extracted word. If the hardware resource is limited, you may limit the vocabulary size up to 10k words/tokens (or even smaller) and the dimension of representations up to 256.\n",
    "\n",
    "The following tracks can be developed independently. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "### Track 3.1 (15%): Use N-gram Language Modeling\n",
    "\n",
    "N-gram Language Modeling is to predict a target word by using `n` words from previous context. Specifically,\n",
    "\n",
    "$P(w_i | w_{i-1}, w_{i-2}, ..., w_{i-n+1})$\n",
    "\n",
    "For example, given a sentence, `\"the main symptoms of COVID-19 are fever and cough\"`, if $n=7$, we use previous context `[\"the\", \"main\", \"symptoms\", \"of\", \"COVID-19\", \"are\"]` to predict the next word `\"fever\"`.\n",
    "\n",
    "More to read: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.2 (15%): Use Skip-gram with Negative Sampling\n",
    "\n",
    "In skip-gram, we use a central word to predict its context. Specifically,\n",
    "\n",
    "$P(w_{c-m}, ... w_{c-1}, w_{c+1}, ..., w_{c+m} | w_c)$\n",
    "\n",
    "As the learning objective of skip-gram is computational inefficient (summation of entire vocabulary $|V|$), negative sampling is commonly applied to accelerate the training.\n",
    "\n",
    "In negative sampling, we randomly select one word from the context as a positive sample, and randomly select $K$ words from the vocabulary as negative samples. As a result, the learning objective is updated to\n",
    "\n",
    "$L = -\\log\\sigma(u^T_{t} v_c) - \\sum_{k=1}^K\\log\\sigma(-u^T_k v_c)$, where $u_t$ is the vector embedding of positive sample from context, $u_k$ are the vector embeddings of negative samples, $v_c$ is the vector embedding of the central word, $\\sigma$ refers to the sigmoid function.\n",
    "\n",
    "More to read http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf section 4.3 and 4.4\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.3 (Bonus +5%): Use Contextualised Word Representation by Masked Language Model (MLM)\n",
    "\n",
    "BERT introduces a new language model for pre-training named Masked Language Model (MLM). The advantage of MLM is that the word representations by MLM will be contextualised.\n",
    "\n",
    "For example, \"stick\" may have different meanings in different context. By N-gram language modeling and word2vec (skip-gram, CBOW), the word representation of \"stick\" is fixed regardless of its context. However, MLM will learn the representation of \"stick\" dynamatically based on context. In other words, \"stick\" will have different representations in different context by MLM.\n",
    "\n",
    "More to read: http://jalammar.github.io/illustrated-bert/ and https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- An algorithm that is able to generate contextualised representation in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 输入: tokens.txt文件路径\n",
    "token_file = r\"E:\\NLP\\test_tokens.txt\"\n",
    "# token_file = r\"D:\\Scholar\\AI Winter School 2025\\Project\\Ours\\NLP\\test_tokens.txt\"\n",
    "# 输出: 保存模型的路径\n",
    "model_file = r\"E:\\NLP\\Skip_gram.model\"\n",
    "# model_file = r\"D:\\Scholar\\AI Winter School 2025\\Project\\Ours\\NLP\\test_Skip_gram.model\"\n",
    "\n",
    "\n",
    "# 读取tokens\n",
    "with open(token_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    tokens = f.read().split()\n",
    "\n",
    "# 方法1：使用 N-gram\n",
    "def generate_ngrams(tokens, n):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "# 方法2：使用 Skip-gram\n",
    "def generate_skipgrams(tokens, window_size, vector_size=100, min_count=1, sg=1, workers=6):\n",
    "    # 将 tokens 分成多个句子\n",
    "    sentences = [tokens[i:i+100] for i in range(0, len(tokens), 100)]\n",
    "    model = Word2Vec(vector_size=vector_size, window=window_size, min_count=min_count, sg=sg, workers=workers)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "# 示例使用\n",
    "n = 3  # N-gram 的 n 值\n",
    "window_size = 2  # Skip-gram 的窗口大小\n",
    "\n",
    "# 生成 N-gram\n",
    "ngrams = generate_ngrams(tokens, n)\n",
    "print(f\"前10个N-grams单词:\", ngrams[:10])  # 打印前10个 N-gram\n",
    "\n",
    "# 生成 Skip-gram 模型\n",
    "skipgram_model = generate_skipgrams(tokens, window_size)\n",
    "print(\"前10个Skip-gram单词:\", list(skipgram_model.wv.index_to_key)[:10])  # 打印前10个词汇\n",
    "\n",
    "# 保存模型\n",
    "skipgram_model.save(model_file)\n",
    "print(f\"Model saved to {model_file}\")\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = Word2Vec.load(model_file)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# 示例：获取多个词汇的向量表示\n",
    "word_vectors = loaded_model.wv\n",
    "words = [\"COVID\", \"lockdowns\", \"impact\"]\n",
    "for word in words:\n",
    "    if word in word_vectors:\n",
    "        print(f\"Vector representation of '{word}':\", word_vectors[word])\n",
    "    else:\n",
    "        print(f\"Word '{word}' not in vocabulary\")\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (20%): Explore the Word Representations\n",
    "\n",
    "The following tracks can be finished independently. You are encouraged to divide workload to each team member.\n",
    "\n",
    "### Track 4.1 (5%): Visualise the word representations by t-SNE\n",
    "\n",
    "t-SNE is an algorithm to reduce dimentionality and commonly used to visualise high-dimension vectors. Use t-SNE to visualise the word representations. You may visualise up to 1000 words as t-SNE is highly computationally complex.\n",
    "\n",
    "More about t-SNE: https://lvdmaaten.github.io/tsne/\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram by t-SNE based on representations of up to 1000 words.\n",
    "\n",
    "### Track 4.2 (5%): Visualise the Word Representations of Biomedical Entities by t-SNE\n",
    "\n",
    "Instead of visualising the word representations of the entire vocabulary (or 1000 words that are selected at random), visualise the word representations of words which are biomedical entities. For example, fever, cough, diabetes etc. Based on the category of those biomedical entities, can you assign different colours to the entities and see if the entities from the same category can be clustered by t-SNE? For example, sinusitis and cough are both respirtory diseases so they should be assigned with the same colour and ideally their representations should be close to each other by t-SNE. Another example, Alzheimer and headache are neuralogical diseases which should be assigned by another colour.\n",
    "\n",
    "Examples of biomedial ontology: https://www.ebi.ac.uk/ols/ontologies/hp and https://en.wikipedia.org/wiki/International_Classification_of_Diseases\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram with colours by t-SNE based on representations of biomedical entities.\n",
    "\n",
    "### Track 4.3 (5%): Co-occurrence\n",
    "\n",
    "- What are the biomedical entities which frequently co-occur with COVID-19 (or coronavirus)?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Track 4.4 (5%): Semantic Similarity\n",
    "\n",
    "- What are the biomedical entities which have closest semantic similarity COVID-19 (or coronavirus) based on word representations?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Open Question (Optional): What else can you discover?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import mplcursors\n",
    "\n",
    "# 输入：Skip_gram模型的路径\n",
    "# model_file = r\"D:\\Scholar\\AI Winter School 2025\\Project\\Ours\\NLP\\test_Skip_gram.model\"\n",
    "model_file = r\"E:NLP\\Skip_gram.model\"\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = Word2Vec.load(model_file)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# 获取词汇的向量表示\n",
    "word_vectors = loaded_model.wv\n",
    "\n",
    "# 获取最多1000个单词及其向量\n",
    "words = list(word_vectors.index_to_key)[:1000]\n",
    "vectors = np.array([word_vectors[word] for word in words])\n",
    "\n",
    "# 使用t-SNE降维到3D\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# 可视化\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], reduced_vectors[:, 2])\n",
    "\n",
    "# 添加注释\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2], word)\n",
    "\n",
    "# 添加交互功能\n",
    "mplcursors.cursor(scatter, hover=True)\n",
    "\n",
    "plt.title('3D t-SNE visualization of word vectors')\n",
    "plt.show()\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (Bonus +10%): Open Challenge: Mining Biomedical Knowledge\n",
    "\n",
    "A fundamental task in clinical/biomedical natural language processing is to extract intelligence from biomedical text corpus automatically and efficiently. More specifically, the intelligence may include biomedical entities mentioned in text, relations between biomedical entities, clinical features of patients, progression of diseases, all of which can be used to predict, understand and improve patients' outcomes. \n",
    "\n",
    "This open challenge is to build a biomedical knowledge graph based on the CORD-19 dataset and mine useful information from it. We recommend the following steps but you are also encouraged to develop your solution from scratch.\n",
    "\n",
    "### Extract Biomedical Entities from Text\n",
    "\n",
    "Extract biomedical entities (such as fever, cough, headache, lung cancer, heart attack) from text. Note that:\n",
    "\n",
    "- The biomedical entities may consist of multiple words. For example, heart attack, multiple myeloma etc.\n",
    "- The biomedical entities may be written in synoynms. For example, low blood pressure for hypotension.\n",
    "- The biomedical entities may be written in different forms. For example, smoking, smokes, smoked.\n",
    "\n",
    "### Extract Relations between Biomedical Entities\n",
    "\n",
    "Extract relations between biomedical entities based on their appearance in text. You may define a relation between biomedical entities by one or more of the following criteria:\n",
    "\n",
    "- The biomedical entities frequentely co-occuer together.\n",
    "- The biomedical entities have similar word representations.\n",
    "- The biomedical entities have clear relations based on textual narratives. For example, \"The most common symptoms for COVID-19 are fever and cough\" so we know there are relations between \"COVID-19\", \"fever\" and \"cough\".\n",
    "\n",
    "### Build a Biomedical Knowledge Graph of COVID-19\n",
    "\n",
    "Build a knoweledge graph based on the results from track 5.1 and 5.2 and visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# 输入：Skip_gram模型的路径\n",
    "model_file = r\"D:\\Scholar\\AI Winter School 2025\\Project\\Ours\\NLP\\Skip_gram.model\"\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = Word2Vec.load(model_file)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# 获取词汇的向量表示\n",
    "word_vectors = loaded_model.wv\n",
    "\n",
    "# 定义生物医学实体及其类别\n",
    "biomedical_entities = {\n",
    "    \"fever\": \"Symptoms\",\n",
    "    \"cough\": \"Symptoms\",\n",
    "    \"headache\": \"Symptoms\",\n",
    "    \"seizure\": \"Symptoms\",\n",
    "    \"infection\": \"Symptoms\",\n",
    "    \"inflammation\": \"Symptoms\",\n",
    "    \"nausea\": \"Symptoms\",\n",
    "    \"fatigue\": \"Symptoms\",\n",
    "    \"pain\": \"Symptoms\",\n",
    "    \"diarrhea\": \"Symptoms\",\n",
    "    \"vomiting\": \"Symptoms\",\n",
    "    \"rash\": \"Symptoms\",\n",
    "    \"dizziness\": \"Symptoms\",\n",
    "    \"weakness\": \"Symptoms\",\n",
    "    \"chills\": \"Symptoms\",\n",
    "    \"sore\": \"Symptoms\",\n",
    "    \"swelling\": \"Symptoms\",\n",
    "    \"bleeding\": \"Symptoms\",\n",
    "    \"itching\": \"Symptoms\",\n",
    "    \"sneezing\": \"Symptoms\",\n",
    "    \"pneumonia\": \"Disease\",\n",
    "    \"asthma\": \"Disease\",\n",
    "    \"diabetes\": \"Disease\",\n",
    "    \"stroke\": \"Disease\",\n",
    "    \"COVID\": \"Disease\",\n",
    "    \"SARS\": \"Disease\",\n",
    "    \"influenza\": \"Disease\",\n",
    "    \"dengue\": \"Disease\",\n",
    "    \"cancer\": \"Disease\",\n",
    "    \"tuberculosis\": \"Disease\",\n",
    "    \"arthritis\": \"Disease\",\n",
    "    \"leukemia\": \"Disease\",\n",
    "    \"hepatitis\": \"Disease\",\n",
    "    \"HIV\": \"Disease\",\n",
    "    \"malaria\": \"Disease\",\n",
    "    \"cholera\": \"Disease\",\n",
    "    \"measles\": \"Disease\",\n",
    "    \"mumps\": \"Disease\",\n",
    "    \"rabies\": \"Disease\",\n",
    "    \"syphilis\": \"Disease\",\n",
    "    \"virus\": \"Microorganisms\",\n",
    "    \"bacteria\": \"Microorganisms\",\n",
    "    \"coronavirus\": \"Microorganisms\",\n",
    "    \"rhinovirus\": \"Microorganisms\",\n",
    "    \"adenovirus\": \"Microorganisms\",\n",
    "    \"pathogen\": \"Microorganisms\",\n",
    "    \"fungi\": \"Microorganisms\",\n",
    "    \"parasite\": \"Microorganisms\",\n",
    "    \"protozoa\": \"Microorganisms\",\n",
    "    \"microbe\": \"Microorganisms\",\n",
    "    \"yeast\": \"Microorganisms\",\n",
    "    \"mold\": \"Microorganisms\",\n",
    "    \"bacillus\": \"Microorganisms\",\n",
    "    \"spirochete\": \"Microorganisms\",\n",
    "    \"mycoplasma\": \"Microorganisms\",\n",
    "    \"prion\": \"Microorganisms\",\n",
    "    \"helminth\": \"Microorganisms\",\n",
    "    \"amoeba\": \"Microorganisms\",\n",
    "    \"algae\": \"Microorganisms\",\n",
    "    \"archaea\": \"Microorganisms\",\n",
    "    \"brain\": \"Organ\",\n",
    "    \"neuron\": \"Organ\",\n",
    "    \"liver\": \"Organ\",\n",
    "    \"pancreas\": \"Organ\",\n",
    "    \"lung\": \"Organ\",\n",
    "    \"spleen\": \"Organ\",\n",
    "    \"kidney\": \"Organ\",\n",
    "    \"heart\": \"Organ\",\n",
    "    \"nerve\": \"Organ\",\n",
    "    \"muscle\": \"Organ\",\n",
    "    \"stomach\": \"Organ\",\n",
    "    \"intestine\": \"Organ\",\n",
    "    \"bladder\": \"Organ\",\n",
    "    \"skin\": \"Organ\",\n",
    "    \"bone\": \"Organ\",\n",
    "    \"eye\": \"Organ\",\n",
    "    \"ear\": \"Organ\",\n",
    "    \"tongue\": \"Organ\",\n",
    "    \"throat\": \"Organ\",\n",
    "    \"esophagus\": \"Organ\",\n",
    "}\n",
    "\n",
    "# 获取生物医学实体及其向量\n",
    "words = []\n",
    "vectors = []\n",
    "categories = []\n",
    "for word in biomedical_entities.keys():\n",
    "    if word in word_vectors:\n",
    "        words.append(word)\n",
    "        vectors.append(word_vectors[word])\n",
    "        categories.append(biomedical_entities[word])\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "# 设置 perplexity 参数，确保其小于样本数量\n",
    "perplexity = min(30, len(vectors) - 1)\n",
    "\n",
    "# 使用t-SNE降维到3D\n",
    "tsne_3d = TSNE(n_components=3, random_state=42, perplexity=perplexity)\n",
    "reduced_vectors_3d = tsne_3d.fit_transform(vectors)\n",
    "\n",
    "# 使用 Plotly 可视化3D图\n",
    "fig_3d = px.scatter_3d(\n",
    "    x=reduced_vectors_3d[:, 0],\n",
    "    y=reduced_vectors_3d[:, 1],\n",
    "    z=reduced_vectors_3d[:, 2],\n",
    "    color=categories,\n",
    "    text=words,\n",
    "    labels={'color': 'Category'},\n",
    "    title='3D t-SNE visualization of biomedical entities'\n",
    ")\n",
    "\n",
    "# 保存为 HTML 文件\n",
    "output_file_3d = r\"D:\\Scholar\\AI Winter School 2025\\Project\\Ours\\NLP\\biomedical_entities_3d.html\"\n",
    "pio.write_html(fig_3d, file=output_file_3d, auto_open=True)\n",
    "\n",
    "# 使用t-SNE降维到2D\n",
    "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "reduced_vectors_2d = tsne_2d.fit_transform(vectors)\n",
    "\n",
    "# 定义颜色映射\n",
    "colors = {\n",
    "    \"Symptoms\": \"red\",\n",
    "    \"Disease\": \"blue\",\n",
    "    \"Microorganisms\": \"green\",\n",
    "    \"Organ\": \"purple\",\n",
    "    # 添加更多类别及其颜色\n",
    "}\n",
    "\n",
    "# 使用 Matplotlib 可视化2D图\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(reduced_vectors_2d[i, 0], reduced_vectors_2d[i, 1], color=colors[categories[i]], label=categories[i])\n",
    "    plt.text(reduced_vectors_2d[i, 0], reduced_vectors_2d[i, 1], word)\n",
    "plt.title('2D t-SNE visualization of biomedical entities')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "# 去重图例\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.show()\n",
    "###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
